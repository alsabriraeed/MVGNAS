basic {
  max_span_width = 15         
  no_cuda = false            
  report_frequency = 500      
  gradient_checkpointing = false      

  # Learning-Related Configs                
  epochs = 50      
  transformer_learning_rate = 5e-05
  batch_size = 32
  max_grad_norm = 1.0
  ned_pretrain_epochs = 1     

  # Architecture-Related Configs              
  feature_size = 20
  span_emb_size = 500 #500                                     
  transformer = allenai/scibert_scivocab_cased      

  mention_scorer_ffnn_size = 500 #500                                
  mention_scorer_ffnn_depth = 2

  mention_linker_ffnn_size = 500 #500                              
  mention_linker_ffnn_depth = 2

  # Configs related to view encoding                       
  multi_architectures = False       
  use_viewencoding =  false     
}

with_view_encoding = ${basic} {
  span_emb_size = 512
  dropout_rate = 0.2    
   span_ratio = 0.6                     
  # BiGNN for Prior IE Graph       
  ieg_bignn_dropout = 0.1
  ieg_bignn_hidden_layers = 2
}    
    
# BioBERT                
biobert = ${with_view_encoding} {
  transformer = dmis-lab/biobert-large-cased-v1.1
}

biobert_with_view_encoding = ${biobert} {
  use_external_knowledge = true
}
